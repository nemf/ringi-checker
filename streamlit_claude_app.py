import streamlit as st
import boto3
import json
from datetime import datetime

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Claude Chat",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
CLAUDE_MODELS = {
    "Claude 3.5 Sonnet": {
        "model_id": "anthropic.claude-3-5-sonnet-20240620-v1:0",
        "description": "æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ« - è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«æœ€é©",
        "max_tokens": 8000,
        "icon": "ğŸ§ ",
        "provider": "Anthropic"
    },
    "Claude 3 Sonnet": {
        "model_id": "anthropic.claude-3-sonnet-20240229-v1:0",
        "description": "é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ« - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæ€§èƒ½",
        "max_tokens": 4000,
        "icon": "ğŸ¯",
        "provider": "Anthropic"
    },
    "Claude 3 Haiku": {
        "model_id": "anthropic.claude-3-haiku-20240307-v1:0", 
        "description": "é«˜é€Ÿãƒ»è»½é‡ãƒ¢ãƒ‡ãƒ« - ç°¡å˜ãªã‚¿ã‚¹ã‚¯ã«æœ€é©",
        "max_tokens": 4000,
        "icon": "âš¡",
        "provider": "Anthropic"
    },
    "Nova Pro": {
        "model_id": "amazon.nova-pro-v1:0",
        "description": "Amazonæœ€é«˜æ€§èƒ½ - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ",
        "max_tokens": 5000,
        "icon": "ğŸš€",
        "provider": "Amazon"
    },
    "Nova Lite": {
        "model_id": "amazon.nova-lite-v1:0",
        "description": "Amazonè»½é‡ãƒ¢ãƒ‡ãƒ« - é«˜é€Ÿå‡¦ç†",
        "max_tokens": 4000,
        "icon": "ğŸ’«",
        "provider": "Amazon"
    },
    "Nova Micro": {
        "model_id": "amazon.nova-micro-v1:0",
        "description": "Amazonè¶…è»½é‡ - åŸºæœ¬ã‚¿ã‚¹ã‚¯ç”¨",
        "max_tokens": 3000,
        "icon": "â­",
        "provider": "Amazon"
    }
}

def initialize_bedrock_client():
    """Bedrock Runtime ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
    try:
        return boto3.client(
            service_name='bedrock-runtime',
            region_name='us-east-1'  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´
        )
    except Exception as e:
        st.error(f"AWS æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

def call_claude(client, model_id, prompt, max_tokens=4000, temperature=0.7):
    """Claude ã‚’å‘¼ã³å‡ºã™"""
    
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": max_tokens,
        "temperature": temperature,
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    }
    
    try:
        response = client.invoke_model(
            modelId=model_id,
            body=json.dumps(body),
            contentType='application/json'
        )
        
        response_body = json.loads(response['body'].read())
        return response_body['content'][0]['text']
        
    except Exception as e:
        st.error(f"Claude å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def call_nova(client, model_id, prompt, max_tokens=4000, temperature=0.7):
    """Amazon Nova ã‚’å‘¼ã³å‡ºã™"""
    
    body = {
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "text": prompt
                    }
                ]
            }
        ],
        "inferenceConfig": {
            "max_new_tokens": max_tokens,
            "temperature": temperature
        }
    }
    
    try:
        response = client.invoke_model(
            modelId=model_id,
            body=json.dumps(body),
            contentType='application/json'
        )
        
        response_body = json.loads(response['body'].read())
        return response_body['output']['message']['content'][0]['text']
        
    except Exception as e:
        st.error(f"Nova å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def call_model(client, model_id, provider, prompt, max_tokens=4000, temperature=0.7):
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™"""
    if provider == "Anthropic":
        return call_claude(client, model_id, prompt, max_tokens, temperature)
    elif provider == "Amazon":
        return call_nova(client, model_id, prompt, max_tokens, temperature)
    else:
        st.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
        return None

def main():
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ¤– AI Chat - Claude & Nova")
    st.markdown("---")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        st.subheader("ğŸ¯ ãƒ¢ãƒ‡ãƒ«é¸æŠ")
        selected_model = st.selectbox(
            "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
            options=list(CLAUDE_MODELS.keys()),
            format_func=lambda x: f"{CLAUDE_MODELS[x]['icon']} {x}",
            help="ç”¨é€”ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±è¡¨ç¤º
        model_info = CLAUDE_MODELS[selected_model]
        st.info(f"**{selected_model}** ({model_info['provider']})\n\n{model_info['description']}")
        
        st.markdown("---")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        st.subheader("ğŸ”§ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        max_tokens = st.slider(
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°", 
            100, 
            model_info['max_tokens'], 
            min(4000, model_info['max_tokens'])
        )
        temperature = st.slider("Temperature", 0.0, 1.0, 0.7, 0.1)
        
        st.markdown("---")
        
        # çµ±è¨ˆæƒ…å ±
        if 'messages' in st.session_state and st.session_state.messages:
            st.subheader("ğŸ“Š çµ±è¨ˆ")
            user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
            assistant_messages = len([m for m in st.session_state.messages if m["role"] == "assistant"])
            st.metric("è³ªå•æ•°", user_messages)
            st.metric("å›ç­”æ•°", assistant_messages)
        
        st.markdown("---")
        
        # ä¼šè©±å±¥æ­´ã‚¯ãƒªã‚¢
        if st.button("ğŸ—‘ï¸ ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢", type="secondary"):
            st.session_state.messages = []
            st.rerun()
    
    # Bedrock ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    if 'bedrock_client' not in st.session_state:
        st.session_state.bedrock_client = initialize_bedrock_client()
    
    if st.session_state.bedrock_client is None:
        st.error("AWS Bedrock ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚èªè¨¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.info("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š\n```bash\naws configure\n```")
        return
    
    # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«è¡¨ç¤º
    st.info(f"ç¾åœ¨ä½¿ç”¨ä¸­: {CLAUDE_MODELS[selected_model]['icon']} **{selected_model}** ({CLAUDE_MODELS[selected_model]['provider']})")
    
    # ä¼šè©±å±¥æ­´ã®åˆæœŸåŒ–
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # ä¼šè©±å±¥æ­´ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "timestamp" in message:
                st.caption(f"é€ä¿¡æ™‚åˆ»: {message['timestamp']}")
            if "model" in message and message["role"] == "assistant":
                st.caption(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {message['model']}")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    if prompt := st.chat_input("Claude ã«è³ªå•ã—ã¦ãã ã•ã„..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        st.session_state.messages.append({
            "role": "user", 
            "content": prompt,
            "timestamp": timestamp
        })
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        with st.chat_message("user"):
            st.markdown(prompt)
            st.caption(f"é€ä¿¡æ™‚åˆ»: {timestamp}")
        
        # Claude ã®å¿œç­”ã‚’å–å¾—ãƒ»è¡¨ç¤º
        with st.chat_message("assistant"):
            with st.spinner(f"{selected_model} ãŒè€ƒãˆã¦ã„ã¾ã™..."):
                response = call_model(
                    st.session_state.bedrock_client, 
                    model_info['model_id'],
                    model_info['provider'],
                    prompt, 
                    max_tokens, 
                    temperature
                )
            
            if response:
                st.markdown(response)
                response_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                st.caption(f"å¿œç­”æ™‚åˆ»: {response_timestamp}")
                st.caption(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {selected_model}")
                
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "timestamp": response_timestamp,
                    "model": selected_model
                })
            else:
                st.error("å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: gray;'>
            Powered by Amazon Bedrock | Claude & Nova Models
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
